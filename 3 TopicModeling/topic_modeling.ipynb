{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.clustering import LDA\n",
    "from pyspark.mllib.linalg import Vectors as MLlibVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mobod2022/mob2022013/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt(data_path, min_token_tf, max_token_tf, min_token_length, min_doc_length=50, is_vw_format=False):\n",
    "    time_start = time.time()\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    def parse_file(kv):\n",
    "        line = kv if is_vw_format else kv[1]\n",
    "        line = line.replace('\\n', ' ').replace('\\t', ' ')\n",
    "        \n",
    "        if is_vw_format:\n",
    "            line_list = []\n",
    "            for token in line.split(' ')[1: ]:\n",
    "                lst = token.split(':')\n",
    "                if len(lst) == 1:\n",
    "                    line_list.append(token)\n",
    "                else:\n",
    "                    line_list += [lst[0]] * int(float(lst[1]))\n",
    "            line = ' '.join(line_list)\n",
    "        \n",
    "        for p in string.punctuation:\n",
    "            line = line.replace(p, ' ')\n",
    "        \n",
    "        tokens = [e.strip().lower() for e in line.strip().split(' ') if len(e) > 0]\n",
    "        if is_vw_format:\n",
    "            return tokens\n",
    "        else:\n",
    "            return (kv[0], tokens)\n",
    "\n",
    "\n",
    "    def filter_token(kv):\n",
    "        token = kv[0]\n",
    "        value = kv[1]\n",
    "\n",
    "        if value > max_token_tf or value < min_token_tf:\n",
    "            return False\n",
    "\n",
    "        if len(token) < min_token_length:\n",
    "            return False\n",
    "\n",
    "        if token in stopwords_:\n",
    "            return False\n",
    "\n",
    "        for i in '0123456789':\n",
    "            if i in token:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def get_tokens(tokens):\n",
    "        if is_vw_format:\n",
    "            return tokens\n",
    "        return tokens[1]\n",
    "\n",
    "\n",
    "    def parseVectors(line):\n",
    "        return [int(line[2]), line[0]]\n",
    "\n",
    "\n",
    "    if is_vw_format:\n",
    "        dataset = sc.textFile(data_path)\n",
    "    else:\n",
    "        dataset = sc.wholeTextFiles(\"{}/*\".format(data_path))\n",
    "    dataset = dataset.map(parse_file)\n",
    "    \n",
    "    word_counts = (dataset\n",
    "                   .flatMap(lambda path_with_tokens: ((token, 1) for token in get_tokens(path_with_tokens)))\n",
    "                   .reduceByKey(lambda cnt_1, cnt_2: cnt_1 + cnt_2)\n",
    "                   .sortBy(lambda token_with_cnt: -token_with_cnt[1]))\n",
    "\n",
    "    stopwords_ = set(stopwords.words('english'))\n",
    "\n",
    "    word_counts = word_counts.filter(filter_token)\n",
    "    vocab = set([e[0] for e in word_counts.collect()])\n",
    "\n",
    "    print('Total number of tokens: {}'.format(len(vocab)))\n",
    "    \n",
    "    if is_vw_format:\n",
    "        dataset = (dataset\n",
    "                   .map(lambda kv: (0, list(filter(lambda t: t in vocab, kv))))\n",
    "                   .filter(lambda kv: len(kv[1]) > min_doc_length))\n",
    "    else:\n",
    "        dataset = (dataset\n",
    "                   .map(lambda kv: (kv[0].split('/')[-1], list(filter(lambda t: t in vocab, kv[1]))))\n",
    "                   .filter(lambda kv: len(kv[1]) > min_doc_length))\n",
    "    \n",
    "    print('Total number of documents: {}'.format(dataset.count()))\n",
    "    \n",
    "    data_df = sqlContext.createDataFrame(dataset, ['id', 'tokens'])\n",
    "    \n",
    "    data_df = data_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "    \n",
    "    cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"vectors\")\n",
    "    cv_model = cv.fit(data_df)\n",
    "    df_vect = cv_model.transform(data_df)\n",
    "\n",
    "    bow = (df_vect\n",
    "           .select('vectors', 'tokens', 'id')\n",
    "           .rdd.map(parseVectors)\n",
    "           .mapValues(MLlibVectors.fromML)\n",
    "           .map(list))\n",
    "    \n",
    "    nnz = sum(bow.map(lambda x: list(x[1].values)).reduce(lambda x, y: x + y))\n",
    "    print('Total collection size: {}'.format(nnz))\n",
    "\n",
    "    print('Elapsed time : {} sec.'.format(int(time.time() - time_start)))\n",
    "    return bow, cv_model, nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopicModel:\n",
    "    \n",
    "    def __init__(self, num_topics, cv_model, nnz, num_document_passes, use_phi_broadcast=True, beta=0.0):\n",
    "        self.num_topics = num_topics                    # число тем в модели\n",
    "        self.cv_model_vocabulary = cv_model.vocabulary  # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "        self.nnz = nnz                                  # общее число словопозиций в коллекции\n",
    "        self.num_document_passes = num_document_passes  # число проходов по документу на E-шаге\n",
    "        self.use_phi_broadcast = use_phi_broadcast      # использование бродкастинга матрицы $\\Phi$\n",
    "        self.beta = beta                                # коэффициент регуляризации\n",
    "        self.perplexity_list = []\n",
    "        \n",
    "        phi_wt_np = np.random.random((len(self.cv_model_vocabulary), self.num_topics))\n",
    "        if self.use_phi_broadcast:\n",
    "            self.phi_wt = sc.broadcast(phi_wt_np)\n",
    "        else:\n",
    "            self.phi_wt = phi_wt_np\n",
    "                \n",
    "    def fit(self, bow_data, num_collection_passes=10):\n",
    "        self.perplexity_list = []\n",
    "        time_start = time.time()\n",
    "        for _ in range(num_collection_passes):\n",
    "            \n",
    "            def process_document(document, \n",
    "                                 num_topics=self.num_topics, \n",
    "                                 num_document_passes=self.num_document_passes,\n",
    "                                 phi_wt=self.phi_wt, \n",
    "                                 use_phi_broadcast=self.use_phi_broadcast):\n",
    "                \n",
    "                theta = np.array([1/num_topics] * num_topics)\n",
    "                n_dw = np.zeros(document.size)\n",
    "                n_dw[document.indices] = document.values\n",
    "                \n",
    "                phi = phi_wt.value if use_phi_broadcast else phi_wt\n",
    "                \n",
    "                for _ in range(num_document_passes):\n",
    "                    _p_tdw = np.einsum('wt,t->tw', phi, theta)\n",
    "                    p_tdw = _p_tdw / np.sum(_p_tdw, axis=0, keepdims=True)\n",
    "                    \n",
    "                    _theta = np.einsum('t,wt->w', n_dw, p_tdw)\n",
    "                    theta = _theta / np.sum(_theta, axis=0, keepdims=True)\n",
    "                \n",
    "                n_wt = np.einsum('w,tw->wt', n_dw, p_tdw)\n",
    "                \n",
    "                theta = np.nan_to_num(theta, 0)\n",
    "                \n",
    "                return n_wt, (n_dw * np.log((phi * theta).sum(axis=1))).sum()\n",
    "        \n",
    "            def E_step(rows):\n",
    "                for row in rows:\n",
    "                    _, documents = row\n",
    "                    n_wt, perplexity = process_document(documents)\n",
    "                    yield n_wt, perplexity\n",
    "\n",
    "            E = bow_data.partitionBy(5).mapPartitions(E_step)\n",
    "\n",
    "            n_wt, perplexity = E.reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "            phi = self.phi_wt.value if self.use_phi_broadcast else self.phi_wt\n",
    "            \n",
    "            _phi_wt = np.maximum(n_wt + phi + self.beta, 1e-40)\n",
    "            phi_wt  = _phi_wt / _phi_wt.sum(axis=0, keepdims=True)\n",
    "\n",
    "            if self.use_phi_broadcast:\n",
    "                self.phi_wt = sc.broadcast(phi_wt)\n",
    "            else:\n",
    "                self.phi_wt = phi_wt\n",
    "\n",
    "            self.perplexity_list.append(int(np.exp(-perplexity / self.nnz)))\n",
    "        \n",
    "        print('Elapsed time : {} sec.\\n'.format(int(time.time() - time_start)))\n",
    "        \n",
    "    def print_perplexity(self):\n",
    "        print(self.perplexity_list)\n",
    "    \n",
    "    def print_topics(self, num_tokens=10):\n",
    "        indexes = self.phi_wt.argsort(axis=0)[-num_tokens:][::-1]\n",
    "        for idx, i_w in enumerate(range(indexes.shape[1])):\n",
    "            print('topic_id: {}'.format(idx))\n",
    "            for i in indexes[:, i_w]:\n",
    "                print(model.cv_model_vocabulary[i])\n",
    "            print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/data/mobod/tm/vw.wiki-en-20K.txt\"\n",
    "min_token_tf = 10\n",
    "max_token_tf = 30000\n",
    "min_token_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 42143\n",
      "Total number of documents: 16029\n",
      "Total collection size: 5193241.0\n",
      "Elapsed time : 389 sec.\n"
     ]
    }
   ],
   "source": [
    "bow, cv_model, nnz = read_txt(data_path, \\\n",
    "                              min_token_tf, \\\n",
    "                              max_token_tf, \\\n",
    "                              min_token_length, \\\n",
    "                              min_doc_length=50, \\\n",
    "                              is_vw_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Перед проведением экспериментов, проверим, что данные разбиваются на достаточное число партиций и что среди них нет вырожденных (существенно меньших по объёму, чем прочие)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_row_in_partitions(rows):\n",
    "    count = 0\n",
    "    for _ in rows:\n",
    "        count += 1\n",
    "    yield count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 21.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8038, 7991]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bow.mapPartitions(count_row_in_partitions).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3206, 3206, 3207, 3205, 3205]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bow.partitionBy(5).mapPartitions(count_row_in_partitions).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим время работы при num_document_passes=5 и num_collection_passes=10, без broadcast переменной, с числом тем num_topics = 10, 20, 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num topics:  10\n",
      "Done\n",
      "Elapsed time : 1200 sec.\n",
      "Num topics:  20\n",
      "Done\n",
      "Elapsed time : 1953 sec.\n",
      "Num topics:  50\n",
      "Done\n",
      "Elapsed time : 5161 sec.\n"
     ]
    }
   ],
   "source": [
    "num_topics_list = [10, 20, 50]\n",
    "for num_topics in num_topics_list:\n",
    "    print('Num topics: ', num_topics)\n",
    "    model = TopicModel(num_topics=num_topics,    # число тем в модели\n",
    "                       cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                       nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                       num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                       use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                       beta=0.0)                 # коэффициент регуляризации\n",
    "    model.fit(bow, num_collection_passes=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим время работы при num_document_passes=5 и num_collection_passes=10, с broadcast переменной, с числом тем num_topics = 10, 20, 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num topics:  10\n",
      "Done\n",
      "Elapsed time : 1464 sec.\n",
      "Num topics:  20\n",
      "Done\n",
      "Elapsed time : 2461 sec.\n",
      "Num topics:  50\n",
      "Done\n",
      "Elapsed time : 4760 sec.\n"
     ]
    }
   ],
   "source": [
    "num_topics_list = [10, 20, 50]\n",
    "for num_topics in num_topics_list:\n",
    "    print('Num topics: ', num_topics)\n",
    "    model = TopicModel(num_topics=num_topics,    # число тем в модели\n",
    "                       cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                       nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                       num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                       use_phi_broadcast=True,   # использование бродкастинга матрицы $\\Phi$\n",
    "                       beta=0.0)                 # коэффициент регуляризации\n",
    "    model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сделать вывод о том, что использование broadcast не ускоряет алгоритм для маленького количества тем, но возможно есть небольшой прирост когда тем 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим время работы, перплексию и интерпретируемость тем по топ-словам при num_topics=20 и num_collection_passes=10 и различных значениях num_document_passes = 1, 2, 5, 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_document_passes = 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics=20,            # число тем в модели\n",
    "                   cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                   nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                   num_document_passes=1,    # число проходов по документу на E-шаге\n",
    "                   use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                   beta=0.0)                 # коэффициент регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Elapsed time : 693 sec.\n",
      "CPU times: user 1.07 s, sys: 384 ms, total: 1.46 s\n",
      "Wall time: 11min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_id: 0\n",
      "album\n",
      "club\n",
      "station\n",
      "game\n",
      "king\n",
      "system\n",
      "president\n",
      "park\n",
      "show\n",
      "church\n",
      "--------\n",
      "topic_id: 1\n",
      "album\n",
      "town\n",
      "side\n",
      "air\n",
      "white\n",
      "club\n",
      "district\n",
      "political\n",
      "title\n",
      "division\n",
      "--------\n",
      "topic_id: 2\n",
      "film\n",
      "party\n",
      "town\n",
      "club\n",
      "league\n",
      "children\n",
      "band\n",
      "london\n",
      "book\n",
      "right\n",
      "--------\n",
      "topic_id: 3\n",
      "album\n",
      "league\n",
      "club\n",
      "president\n",
      "political\n",
      "king\n",
      "party\n",
      "french\n",
      "division\n",
      "championship\n",
      "--------\n",
      "topic_id: 4\n",
      "league\n",
      "king\n",
      "town\n",
      "society\n",
      "game\n",
      "district\n",
      "station\n",
      "man\n",
      "song\n",
      "band\n",
      "--------\n",
      "topic_id: 5\n",
      "station\n",
      "town\n",
      "river\n",
      "game\n",
      "church\n",
      "king\n",
      "division\n",
      "children\n",
      "league\n",
      "band\n",
      "--------\n",
      "topic_id: 6\n",
      "film\n",
      "system\n",
      "church\n",
      "king\n",
      "party\n",
      "show\n",
      "album\n",
      "football\n",
      "road\n",
      "band\n",
      "--------\n",
      "topic_id: 7\n",
      "film\n",
      "album\n",
      "party\n",
      "town\n",
      "league\n",
      "king\n",
      "station\n",
      "society\n",
      "london\n",
      "song\n",
      "--------\n",
      "topic_id: 8\n",
      "film\n",
      "station\n",
      "game\n",
      "band\n",
      "football\n",
      "research\n",
      "law\n",
      "center\n",
      "published\n",
      "division\n",
      "--------\n",
      "topic_id: 9\n",
      "film\n",
      "league\n",
      "church\n",
      "game\n",
      "king\n",
      "air\n",
      "road\n",
      "london\n",
      "football\n",
      "women\n",
      "--------\n",
      "topic_id: 10\n",
      "film\n",
      "album\n",
      "party\n",
      "town\n",
      "right\n",
      "band\n",
      "park\n",
      "son\n",
      "man\n",
      "song\n",
      "--------\n",
      "topic_id: 11\n",
      "film\n",
      "game\n",
      "system\n",
      "william\n",
      "album\n",
      "son\n",
      "road\n",
      "linear\n",
      "river\n",
      "building\n",
      "--------\n",
      "topic_id: 12\n",
      "party\n",
      "film\n",
      "street\n",
      "league\n",
      "king\n",
      "building\n",
      "development\n",
      "published\n",
      "football\n",
      "album\n",
      "--------\n",
      "topic_id: 13\n",
      "game\n",
      "river\n",
      "system\n",
      "army\n",
      "women\n",
      "society\n",
      "man\n",
      "player\n",
      "court\n",
      "site\n",
      "--------\n",
      "topic_id: 14\n",
      "game\n",
      "district\n",
      "show\n",
      "party\n",
      "works\n",
      "club\n",
      "french\n",
      "law\n",
      "england\n",
      "air\n",
      "--------\n",
      "topic_id: 15\n",
      "church\n",
      "game\n",
      "district\n",
      "children\n",
      "song\n",
      "students\n",
      "army\n",
      "league\n",
      "son\n",
      "cup\n",
      "--------\n",
      "topic_id: 16\n",
      "party\n",
      "church\n",
      "album\n",
      "road\n",
      "show\n",
      "river\n",
      "president\n",
      "book\n",
      "works\n",
      "linear\n",
      "--------\n",
      "topic_id: 17\n",
      "film\n",
      "album\n",
      "church\n",
      "town\n",
      "club\n",
      "party\n",
      "street\n",
      "band\n",
      "river\n",
      "cup\n",
      "--------\n",
      "topic_id: 18\n",
      "film\n",
      "league\n",
      "station\n",
      "system\n",
      "road\n",
      "water\n",
      "white\n",
      "research\n",
      "club\n",
      "town\n",
      "--------\n",
      "topic_id: 19\n",
      "street\n",
      "band\n",
      "song\n",
      "station\n",
      "town\n",
      "park\n",
      "law\n",
      "women\n",
      "film\n",
      "president\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 11692, 11588, 11588, 11588, 11588, 11588, 11588, 11588, 11588]\n"
     ]
    }
   ],
   "source": [
    "model.print_perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_document_passes = 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics=20,            # число тем в модели\n",
    "                   cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                   nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                   num_document_passes=2,    # число проходов по документу на E-шаге\n",
    "                   use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                   beta=0.0)                 # коэффициент регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Elapsed time : 1095 sec.\n",
      "CPU times: user 1.12 s, sys: 368 ms, total: 1.48 s\n",
      "Wall time: 18min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_id: 0\n",
      "film\n",
      "james\n",
      "band\n",
      "robert\n",
      "london\n",
      "business\n",
      "village\n",
      "book\n",
      "song\n",
      "george\n",
      "--------\n",
      "topic_id: 1\n",
      "king\n",
      "emperor\n",
      "film\n",
      "empire\n",
      "roman\n",
      "album\n",
      "china\n",
      "province\n",
      "show\n",
      "award\n",
      "--------\n",
      "topic_id: 2\n",
      "station\n",
      "railway\n",
      "road\n",
      "center\n",
      "district\n",
      "park\n",
      "street\n",
      "central\n",
      "town\n",
      "party\n",
      "--------\n",
      "topic_id: 3\n",
      "club\n",
      "band\n",
      "song\n",
      "album\n",
      "live\n",
      "video\n",
      "party\n",
      "game\n",
      "championship\n",
      "record\n",
      "--------\n",
      "topic_id: 4\n",
      "league\n",
      "cup\n",
      "football\n",
      "round\n",
      "division\n",
      "club\n",
      "park\n",
      "ret\n",
      "goals\n",
      "games\n",
      "--------\n",
      "topic_id: 5\n",
      "album\n",
      "film\n",
      "band\n",
      "song\n",
      "award\n",
      "show\n",
      "radio\n",
      "love\n",
      "chart\n",
      "video\n",
      "--------\n",
      "topic_id: 6\n",
      "linear\n",
      "socorro\n",
      "peak\n",
      "saint\n",
      "kitt\n",
      "spacewatch\n",
      "anderson\n",
      "mount\n",
      "system\n",
      "neat\n",
      "--------\n",
      "topic_id: 7\n",
      "church\n",
      "king\n",
      "president\n",
      "art\n",
      "german\n",
      "army\n",
      "military\n",
      "air\n",
      "radio\n",
      "force\n",
      "--------\n",
      "topic_id: 8\n",
      "education\n",
      "research\n",
      "open\n",
      "australia\n",
      "medal\n",
      "club\n",
      "art\n",
      "society\n",
      "network\n",
      "women\n",
      "--------\n",
      "topic_id: 9\n",
      "saint\n",
      "cause\n",
      "party\n",
      "system\n",
      "right\n",
      "political\n",
      "center\n",
      "river\n",
      "stadium\n",
      "william\n",
      "--------\n",
      "topic_id: 10\n",
      "party\n",
      "development\n",
      "aircraft\n",
      "air\n",
      "court\n",
      "cup\n",
      "player\n",
      "church\n",
      "version\n",
      "game\n",
      "--------\n",
      "topic_id: 11\n",
      "regiment\n",
      "foot\n",
      "river\n",
      "battalion\n",
      "indian\n",
      "infantry\n",
      "game\n",
      "army\n",
      "point\n",
      "head\n",
      "--------\n",
      "topic_id: 12\n",
      "trust\n",
      "party\n",
      "nhs\n",
      "health\n",
      "election\n",
      "community\n",
      "development\n",
      "education\n",
      "building\n",
      "law\n",
      "--------\n",
      "topic_id: 13\n",
      "village\n",
      "jpg\n",
      "system\n",
      "film\n",
      "church\n",
      "lake\n",
      "washington\n",
      "file\n",
      "william\n",
      "using\n",
      "--------\n",
      "topic_id: 14\n",
      "president\n",
      "william\n",
      "district\n",
      "book\n",
      "cause\n",
      "california\n",
      "law\n",
      "san\n",
      "building\n",
      "union\n",
      "--------\n",
      "topic_id: 15\n",
      "album\n",
      "game\n",
      "film\n",
      "song\n",
      "white\n",
      "radio\n",
      "release\n",
      "record\n",
      "show\n",
      "award\n",
      "--------\n",
      "topic_id: 16\n",
      "castle\n",
      "chateau\n",
      "film\n",
      "game\n",
      "london\n",
      "park\n",
      "isbn\n",
      "population\n",
      "story\n",
      "party\n",
      "--------\n",
      "topic_id: 17\n",
      "party\n",
      "miss\n",
      "district\n",
      "river\n",
      "building\n",
      "william\n",
      "james\n",
      "community\n",
      "show\n",
      "democratic\n",
      "--------\n",
      "topic_id: 18\n",
      "royal\n",
      "game\n",
      "london\n",
      "league\n",
      "games\n",
      "field\n",
      "land\n",
      "women\n",
      "next\n",
      "book\n",
      "--------\n",
      "topic_id: 19\n",
      "bgcolor\n",
      "win\n",
      "gold\n",
      "programs\n",
      "width\n",
      "game\n",
      "football\n",
      "news\n",
      "championship\n",
      "dehydrogenase\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11628, 11488, 11430, 11353, 11260, 11144, 10993, 10795, 10531]\n"
     ]
    }
   ],
   "source": [
    "model.print_perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_document_passes = 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics=20,            # число тем в модели\n",
    "                   cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                   nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                   num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                   use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                   beta=0.0)                 # коэффициент регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Elapsed time : 1940 sec.\n",
      "CPU times: user 1.22 s, sys: 384 ms, total: 1.6 s\n",
      "Wall time: 32min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_id: 0\n",
      "water\n",
      "species\n",
      "research\n",
      "river\n",
      "often\n",
      "region\n",
      "common\n",
      "form\n",
      "level\n",
      "areas\n",
      "--------\n",
      "topic_id: 1\n",
      "club\n",
      "division\n",
      "league\n",
      "japan\n",
      "town\n",
      "station\n",
      "football\n",
      "win\n",
      "manchester\n",
      "railway\n",
      "--------\n",
      "topic_id: 2\n",
      "church\n",
      "saint\n",
      "town\n",
      "village\n",
      "french\n",
      "trust\n",
      "community\n",
      "paris\n",
      "institute\n",
      "museum\n",
      "--------\n",
      "topic_id: 3\n",
      "cup\n",
      "league\n",
      "club\n",
      "stadium\n",
      "football\n",
      "round\n",
      "goals\n",
      "championship\n",
      "total\n",
      "match\n",
      "--------\n",
      "topic_id: 4\n",
      "students\n",
      "children\n",
      "women\n",
      "news\n",
      "program\n",
      "radio\n",
      "president\n",
      "society\n",
      "human\n",
      "media\n",
      "--------\n",
      "topic_id: 5\n",
      "air\n",
      "army\n",
      "force\n",
      "aircraft\n",
      "military\n",
      "fire\n",
      "forces\n",
      "battle\n",
      "training\n",
      "command\n",
      "--------\n",
      "topic_id: 6\n",
      "isbn\n",
      "book\n",
      "published\n",
      "books\n",
      "modern\n",
      "theory\n",
      "god\n",
      "written\n",
      "text\n",
      "volume\n",
      "--------\n",
      "topic_id: 7\n",
      "party\n",
      "king\n",
      "china\n",
      "roman\n",
      "political\n",
      "law\n",
      "empire\n",
      "democratic\n",
      "province\n",
      "hong\n",
      "--------\n",
      "topic_id: 8\n",
      "race\n",
      "points\n",
      "indian\n",
      "racing\n",
      "india\n",
      "miss\n",
      "car\n",
      "championship\n",
      "women\n",
      "event\n",
      "--------\n",
      "topic_id: 9\n",
      "system\n",
      "design\n",
      "using\n",
      "designed\n",
      "model\n",
      "type\n",
      "standard\n",
      "file\n",
      "light\n",
      "example\n",
      "--------\n",
      "topic_id: 10\n",
      "album\n",
      "song\n",
      "show\n",
      "episode\n",
      "film\n",
      "video\n",
      "chart\n",
      "records\n",
      "live\n",
      "love\n",
      "--------\n",
      "topic_id: 11\n",
      "game\n",
      "san\n",
      "california\n",
      "chicago\n",
      "washington\n",
      "league\n",
      "texas\n",
      "baseball\n",
      "florida\n",
      "center\n",
      "--------\n",
      "topic_id: 12\n",
      "band\n",
      "castle\n",
      "men\n",
      "tour\n",
      "chateau\n",
      "poetry\n",
      "poet\n",
      "art\n",
      "guitar\n",
      "rock\n",
      "--------\n",
      "topic_id: 13\n",
      "game\n",
      "games\n",
      "art\n",
      "space\n",
      "magazine\n",
      "player\n",
      "characters\n",
      "directed\n",
      "japanese\n",
      "character\n",
      "--------\n",
      "topic_id: 14\n",
      "film\n",
      "role\n",
      "father\n",
      "director\n",
      "regiment\n",
      "court\n",
      "foot\n",
      "married\n",
      "royal\n",
      "cast\n",
      "--------\n",
      "topic_id: 15\n",
      "film\n",
      "award\n",
      "love\n",
      "story\n",
      "blue\n",
      "matter\n",
      "jack\n",
      "starring\n",
      "festival\n",
      "jazz\n",
      "--------\n",
      "topic_id: 16\n",
      "linear\n",
      "socorro\n",
      "election\n",
      "party\n",
      "peak\n",
      "district\n",
      "president\n",
      "kitt\n",
      "spacewatch\n",
      "bgcolor\n",
      "--------\n",
      "topic_id: 17\n",
      "william\n",
      "george\n",
      "london\n",
      "sir\n",
      "thomas\n",
      "henry\n",
      "james\n",
      "business\n",
      "cause\n",
      "charles\n",
      "--------\n",
      "topic_id: 18\n",
      "road\n",
      "park\n",
      "station\n",
      "street\n",
      "river\n",
      "building\n",
      "site\n",
      "route\n",
      "railway\n",
      "bridge\n",
      "--------\n",
      "topic_id: 19\n",
      "island\n",
      "ship\n",
      "union\n",
      "sea\n",
      "land\n",
      "islands\n",
      "french\n",
      "soviet\n",
      "ships\n",
      "military\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11368, 10888, 10184, 9102, 7937, 7037, 6439, 6050, 5789]\n"
     ]
    }
   ],
   "source": [
    "model.print_perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_document_passes = 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics=20,            # число тем в модели\n",
    "                   cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                   nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                   num_document_passes=10,   # число проходов по документу на E-шаге\n",
    "                   use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                   beta=0.0)                 # коэффициент регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Elapsed time : 3609 sec.\n",
      "CPU times: user 1.32 s, sys: 544 ms, total: 1.86 s\n",
      "Wall time: 1h 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_id: 0\n",
      "species\n",
      "miss\n",
      "polish\n",
      "little\n",
      "black\n",
      "white\n",
      "female\n",
      "mexico\n",
      "german\n",
      "male\n",
      "--------\n",
      "topic_id: 1\n",
      "king\n",
      "roman\n",
      "father\n",
      "cause\n",
      "opera\n",
      "mother\n",
      "son\n",
      "man\n",
      "empire\n",
      "daughter\n",
      "--------\n",
      "topic_id: 2\n",
      "film\n",
      "show\n",
      "episode\n",
      "television\n",
      "award\n",
      "radio\n",
      "awards\n",
      "role\n",
      "director\n",
      "films\n",
      "--------\n",
      "topic_id: 3\n",
      "india\n",
      "indian\n",
      "population\n",
      "region\n",
      "island\n",
      "africa\n",
      "water\n",
      "islands\n",
      "land\n",
      "sea\n",
      "--------\n",
      "topic_id: 4\n",
      "game\n",
      "player\n",
      "games\n",
      "league\n",
      "win\n",
      "players\n",
      "championship\n",
      "play\n",
      "champion\n",
      "round\n",
      "--------\n",
      "topic_id: 5\n",
      "air\n",
      "aircraft\n",
      "army\n",
      "force\n",
      "navy\n",
      "engine\n",
      "car\n",
      "ship\n",
      "racing\n",
      "military\n",
      "--------\n",
      "topic_id: 6\n",
      "system\n",
      "information\n",
      "systems\n",
      "data\n",
      "access\n",
      "development\n",
      "health\n",
      "water\n",
      "using\n",
      "process\n",
      "--------\n",
      "topic_id: 7\n",
      "book\n",
      "isbn\n",
      "published\n",
      "books\n",
      "story\n",
      "man\n",
      "magazine\n",
      "press\n",
      "novel\n",
      "stories\n",
      "--------\n",
      "topic_id: 8\n",
      "album\n",
      "band\n",
      "song\n",
      "records\n",
      "songs\n",
      "chart\n",
      "video\n",
      "guitar\n",
      "track\n",
      "live\n",
      "--------\n",
      "topic_id: 9\n",
      "saint\n",
      "french\n",
      "art\n",
      "france\n",
      "paris\n",
      "des\n",
      "bgcolor\n",
      "works\n",
      "museum\n",
      "jean\n",
      "--------\n",
      "topic_id: 10\n",
      "castle\n",
      "prince\n",
      "son\n",
      "emperor\n",
      "chateau\n",
      "chinese\n",
      "army\n",
      "military\n",
      "china\n",
      "king\n",
      "--------\n",
      "topic_id: 11\n",
      "system\n",
      "space\n",
      "model\n",
      "using\n",
      "power\n",
      "different\n",
      "example\n",
      "standard\n",
      "point\n",
      "form\n",
      "--------\n",
      "topic_id: 12\n",
      "league\n",
      "club\n",
      "cup\n",
      "football\n",
      "division\n",
      "stadium\n",
      "goals\n",
      "regiment\n",
      "round\n",
      "championship\n",
      "--------\n",
      "topic_id: 13\n",
      "river\n",
      "station\n",
      "road\n",
      "park\n",
      "town\n",
      "railway\n",
      "village\n",
      "lake\n",
      "route\n",
      "highway\n",
      "--------\n",
      "topic_id: 14\n",
      "business\n",
      "million\n",
      "services\n",
      "management\n",
      "market\n",
      "companies\n",
      "kong\n",
      "development\n",
      "financial\n",
      "hong\n",
      "--------\n",
      "topic_id: 15\n",
      "party\n",
      "election\n",
      "political\n",
      "president\n",
      "union\n",
      "council\n",
      "democratic\n",
      "elected\n",
      "canada\n",
      "parties\n",
      "--------\n",
      "topic_id: 16\n",
      "building\n",
      "linear\n",
      "socorro\n",
      "street\n",
      "peak\n",
      "museum\n",
      "hall\n",
      "jpg\n",
      "kitt\n",
      "spacewatch\n",
      "--------\n",
      "topic_id: 17\n",
      "police\n",
      "william\n",
      "london\n",
      "sir\n",
      "court\n",
      "lord\n",
      "church\n",
      "said\n",
      "thomas\n",
      "battle\n",
      "--------\n",
      "topic_id: 18\n",
      "washington\n",
      "texas\n",
      "california\n",
      "baseball\n",
      "chicago\n",
      "district\n",
      "virginia\n",
      "carolina\n",
      "field\n",
      "san\n",
      "--------\n",
      "topic_id: 19\n",
      "students\n",
      "research\n",
      "education\n",
      "society\n",
      "science\n",
      "institute\n",
      "language\n",
      "church\n",
      "professor\n",
      "social\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10917, 9608, 7920, 6673, 6002, 5629, 5402, 5254, 5153]\n"
     ]
    }
   ],
   "source": [
    "model.print_perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сделать ожидаемый вывод о том, что время работы увеличивается, а перплексия снижается с увеличением числа проходов по документу.\n",
    "Что касается интерпретируемости тем, то для 1 и 2 проходов её оказывается недостаточно, хотя для 2 проходов уже можно сделать вывод об одной теме, но бывают выбросы. Разница между 5 и 10 проходами не существенна, поэтому оптимальным числом проходов будет 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим простейшую регуляризацию а-ля LDA, с помощью разреживания константой beta = 0.0, -0.1, -1.0, при параметрах num_topics=20, num_collection_passes=10 и num_document_passes=5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta = 0.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics=20,            # число тем в модели\n",
    "                   cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                   nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                   num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                   use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                   beta=0.0)                 # коэффициент регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time : 2758 sec.\n",
      "\n",
      "CPU times: user 1.18 s, sys: 292 ms, total: 1.47 s\n",
      "Wall time: 45min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11376, 10890, 10138, 8989, 7807, 6936, 6361, 5979, 5722]\n"
     ]
    }
   ],
   "source": [
    "model.print_perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cчитаем разряженность матрицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity:  92.0 %\n"
     ]
    }
   ],
   "source": [
    "sparsity = np.round(100 * (1.0 - np.count_nonzero(np.round(model.phi_wt, decimals=4)) / float(model.phi_wt.size)), decimals=1)\n",
    "print('sparsity: ', sparsity, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta = -0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics=20,            # число тем в модели\n",
    "                   cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                   nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                   num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                   use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                   beta=-0.1)                # коэффициент регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time : 2534 sec.\n",
      "\n",
      "CPU times: user 1.16 s, sys: 320 ms, total: 1.48 s\n",
      "Wall time: 42min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11334, 10883, 10151, 9012, 7784, 6877, 6316, 5969, 5747]\n"
     ]
    }
   ],
   "source": [
    "model.print_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity:  91.9 %\n"
     ]
    }
   ],
   "source": [
    "sparsity = np.round(100 * (1.0 - np.count_nonzero(np.round(model.phi_wt, decimals=4)) / float(model.phi_wt.size)), decimals=1)\n",
    "print('sparsity: ', sparsity, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta = -1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics=20,            # число тем в модели\n",
    "                   cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                   nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                   num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                   use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                   beta=-1.0)                # коэффициент регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time : 2553 sec.\n",
      "\n",
      "CPU times: user 1 s, sys: 280 ms, total: 1.28 s\n",
      "Wall time: 42min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11262, 10849, 9839, 8826, 7914, 7264, 6858, 6605, 6443]\n"
     ]
    }
   ],
   "source": [
    "model.print_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity:  91.6 %\n"
     ]
    }
   ],
   "source": [
    "sparsity = np.round(100 * (1.0 - np.count_nonzero(np.round(model.phi_wt, decimals=4)) / float(model.phi_wt.size)), decimals=1)\n",
    "print('sparsity: ', sparsity, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перплексия увеличивается с увеличением параметра beta, как и разреженность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta = -25.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModel(num_topics=20,            # число тем в модели\n",
    "                   cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                   nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                   num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                   use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                   beta=-25.0)               # коэффициент регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time : 2208 sec.\n",
      "\n",
      "CPU times: user 872 ms, sys: 420 ms, total: 1.29 s\n",
      "Wall time: 36min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(bow, num_collection_passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 96265710347577568, 74498027578544560, 55643859199365960, 35748269860209220, 20201163991965068, 12487707418476852, 8943836361390868, 6928327427149308, 5817054592067324]\n"
     ]
    }
   ],
   "source": [
    "model.print_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity:  98.7 %\n"
     ]
    }
   ],
   "source": [
    "sparsity = np.round(100 * (1.0 - np.count_nonzero(np.round(model.phi_wt, decimals=4)) / float(model.phi_wt.size)), decimals=1)\n",
    "print('sparsity: ', sparsity, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим реализацию структуры для хранения счётчиков и матрицу Φ с numpy-массива на структуру из scipy для работы с разреженными матрицами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopicModelSparse:\n",
    "\n",
    "    def __init__(self, num_topics, cv_model, nnz, num_document_passes, use_phi_broadcast=True, beta=0.0):\n",
    "        self.num_topics = num_topics                    \n",
    "        self.cv_model_vocabulary = cv_model.vocabulary  \n",
    "        self.nnz = nnz                                  \n",
    "        self.num_document_passes = num_document_passes  \n",
    "        self.use_phi_broadcast = use_phi_broadcast      \n",
    "        self.beta = beta                                \n",
    "        self.perplexity_list = []\n",
    "        \n",
    "        phi_wt_csr = csr_matrix(np.random.random((len(self.cv_model_vocabulary), self.num_topics)))\n",
    "        if self.use_phi_broadcast:\n",
    "            self.phiwt = sc.broadcast(phi_wt_csr)\n",
    "        else:\n",
    "            self.phiwt = phi_wt_csr\n",
    "                \n",
    "    def fit(self, bow_data, num_collection_passes=10):\n",
    "        self.perplexity_list = []\n",
    "        time_start = time.time()\n",
    "        for _ in range(num_collection_passes):\n",
    "            \n",
    "            def process_document(document, \n",
    "                                 num_topics=self.num_topics, \n",
    "                                 num_document_passes=self.num_document_passes,\n",
    "                                 phiwt=self.phiwt, \n",
    "                                 use_phi_broadcast=self.use_phi_broadcast):\n",
    "                \n",
    "                theta = csr_matrix(np.array([1/num_topics] * num_topics))\n",
    "                n_dw = np.zeros(document.size)\n",
    "                n_dw[document.indices] = document.values\n",
    "                n_dw = csr_matrix(n_dw)\n",
    "                \n",
    "                phi = phiwt.value if use_phi_broadcast else phiwt\n",
    "                \n",
    "                for _ in range(num_document_passes):\n",
    "                    #_p_tdw = np.einsum('wt,t->tw', phi, theta)\n",
    "                    _p_tdw = (phi.multiply(theta)).T\n",
    "                    p_tdw = _p_tdw / _p_tdw.sum(axis=0)\n",
    "                    #_theta = np.einsum('t,wt->w', n_dw, p_tdw)\n",
    "                    _theta = n_dw.dot(p_tdw.T)\n",
    "                    theta = _theta / _theta.sum(axis=0)\n",
    "                #n_wt = np.einsum('w,tw->wt', n_dw, p_tdw)\n",
    "                n_wt = (p_tdw.multiply(n_dw)).T\n",
    "                \n",
    "                theta = np.nan_to_num(theta, 0)\n",
    "                \n",
    "                return n_wt, (n_dw.toarray() * np.log((phi.multiply(theta)).sum(axis=1))).sum()\n",
    "        \n",
    "            def E_step(rows):\n",
    "                for row in rows:\n",
    "                    _, documents = row\n",
    "                    n_wt, perplexity = process_document(documents)\n",
    "                    yield n_wt, perplexity\n",
    "\n",
    "            E = bow_data.partitionBy(5).mapPartitions(E_step)\n",
    "\n",
    "\n",
    "            n_wt, perplexity = E.reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "            phi = self.phiwt.value if self.use_phi_broadcast else self.phiwt\n",
    "            \n",
    "            _phi_wt = np.maximum(n_wt.todense() + phi.todense() + self.beta, 1e-40)\n",
    "            phiwt  = csr_matrix(_phi_wt / _phi_wt.sum(axis=0))\n",
    "\n",
    "            if self.use_phi_broadcast:\n",
    "                self.phiwt = sc.broadcast(phiwt)\n",
    "            else:\n",
    "                self.phiwt = phiwt\n",
    "\n",
    "\n",
    "            self.perplexity_list.append(int(np.exp(-perplexity / (self.nnz + 1e-40))))\n",
    "        \n",
    "        print('Elapsed time : {} sec.\\n'.format(int(time.time() - time_start)))\n",
    "        \n",
    "    def print_perplexity(self):\n",
    "        print(self.perplexity_list)\n",
    "    \n",
    "    def print_topics(self, num_tokens=10):\n",
    "        indexes = self.phiwt.argsort(axis=0)[-num_tokens:][::-1]\n",
    "        for idx, i_w in enumerate(range(indexes.shape[1])):\n",
    "            print('topic_id: {}'.format(idx))\n",
    "            for i in indexes[:, i_w]:\n",
    "                print(model.cv_model_vocabulary[i])\n",
    "            print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num topics:  10\n",
      "Elapsed time : 1454 sec.\n",
      "\n",
      "[2, 347415074773276608, 289322479036779136, 268359768369659360, 246187881608650208, 217907687035157312, 202492230523938720, 187065982155361216, 175129898402068000, 166953857670722240]\n",
      "Num topics:  20\n",
      "Elapsed time : 2299 sec.\n",
      "\n",
      "[1, 43639416790438482280448, 35206300837285222940672, 32543626795818739040256, 27056635451614492098560, 19475061364181778300928, 15029164789962184327168, 11855623453586166382592, 10058371743066716372992, 8895359532209015881728]\n",
      "Num topics:  50\n",
      "Elapsed time : 4315 sec.\n",
      "\n",
      "[1, 4930901589741260510003590070272, 843449813812448929969275404288, 1604702367895, 1474799926561, 1430009871892, 1399796395377, 1390095885347, 1385804100251, 1376299168296]\n"
     ]
    }
   ],
   "source": [
    "num_topics_list = [10, 20, 50]\n",
    "for num_topics in num_topics_list:\n",
    "    print('Num topics: ', num_topics)\n",
    "    model = TopicModel(num_topics=num_topics,    # число тем в модели\n",
    "                       cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                       nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                       num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                       use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                       beta=-50.0)               # коэффициент регуляризации\n",
    "    model.fit(bow, num_collection_passes=10)\n",
    "    model.print_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num topics:  10\n",
      "Elapsed time : 1072 sec.\n",
      "\n",
      "[2, 9519996327750, 7827064885036, 7194103247180, 6573907893997, 5755058668682, 4964189431759, 4412839792071, 4039596268401, 3689146990891]\n",
      "Num topics:  20\n",
      "Elapsed time : 1916 sec.\n",
      "\n",
      "[1, 83813007954602816, 65030323624663424, 52364805260211864, 37477876803085920, 23539375085451428, 14395727243317482, 9623690957252396, 7516768122379486, 6312555008361182]\n",
      "Num topics:  50\n",
      "Elapsed time : 4596 sec.\n",
      "\n",
      "[1, 608460637590711856791552, 434890635895124380352512, 58106010018199044620288, 94450936673882144768, 4060532980514948608, 2262328083508298240, 1875518993904480256, 1744668108264572672, 1611464321411701248]\n"
     ]
    }
   ],
   "source": [
    "num_topics_list = [10, 20, 50]\n",
    "for num_topics in num_topics_list:\n",
    "    print('Num topics: ', num_topics)\n",
    "    model = TopicModel(num_topics=num_topics,    # число тем в модели\n",
    "                       cv_model=cv_model,        # векторизатор, объект класса `pyspark.ml.feature.CountVectorizer`\n",
    "                       nnz=nnz,                  # общее число словопозиций в коллекции\n",
    "                       num_document_passes=5,    # число проходов по документу на E-шаге\n",
    "                       use_phi_broadcast=False,  # использование бродкастинга матрицы $\\Phi$\n",
    "                       beta=-25.0)               # коэффициент регуляризации\n",
    "    model.fit(bow, num_collection_passes=10)\n",
    "    model.print_perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
